{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# ğŸŒ¸ Iris Flower Classification\n",
    "## CodeAlpha Data Science Internship â€” Task 1\n",
    "\n",
    "---\n",
    "\n",
    "> **Objective:** Build and compare multiple machine learning classifiers to predict the species of Iris flowers based on their physical measurements.\n",
    "\n",
    "| Detail | Info |\n",
    "|--------|------|\n",
    "| **Intern** | Your Name |\n",
    "| **Company** | CodeAlpha |\n",
    "| **Task** | Task 1 â€” Iris Flower Classification |\n",
    "| **Domain** | Supervised ML â€” Multi-class Classification |\n",
    "| **Dataset** | UCI Iris Dataset (via Scikit-learn) |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Table of Contents\n",
    "1. [Import Libraries](#1)\n",
    "2. [Load & Explore Dataset](#2)\n",
    "3. [Exploratory Data Analysis (EDA)](#3)\n",
    "4. [Data Preprocessing](#4)\n",
    "5. [Model Building â€” 4 Classifiers](#5)\n",
    "6. [Model Comparison & Evaluation](#6)\n",
    "7. [Best Model â€” Deep Dive](#7)\n",
    "8. [Conclusion & Insights](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“¦ 1. Import Libraries\n",
    "We import all necessary libraries for data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Core Libraries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# â”€â”€ Visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "# â”€â”€ Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# â”€â”€ Preprocessing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# â”€â”€ Models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# â”€â”€ Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# â”€â”€ Style Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "COLORS = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6']\n",
    "\n",
    "print('âœ… All libraries imported successfully!')\n",
    "print(f'   NumPy     : {np.__version__}')\n",
    "print(f'   Pandas    : {pd.__version__}')\n",
    "print(f'   Sklearn   : {__import__(\"sklearn\").__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“‚ 2. Load & Explore Dataset\n",
    "The **Iris dataset** is one of the most well-known datasets in machine learning.\n",
    "It contains **150 samples** of iris flowers from **3 species**, each described by **4 features**.\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| `sepal_length` | Length of the sepal (cm) |\n",
    "| `sepal_width` | Width of the sepal (cm) |\n",
    "| `petal_length` | Length of the petal (cm) |\n",
    "| `petal_width` | Width of the petal (cm) |\n",
    "| `species` | **Target** â€” Setosa / Versicolor / Virginica |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset from Scikit-learn\n",
    "iris = load_iris()\n",
    "\n",
    "# Convert to a clean Pandas DataFrame\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
    "\n",
    "print('ğŸ“Š Dataset Shape:', df.shape)\n",
    "print('ğŸŒ¸ Target Classes:', iris.target_names.tolist())\n",
    "print()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types, null values and basic statistics\n",
    "print('=' * 50)\n",
    "print('ğŸ“‹ Dataset Info')\n",
    "print('=' * 50)\n",
    "df.info()\n",
    "print()\n",
    "print('=' * 50)\n",
    "print('â“ Missing Values')\n",
    "print('=' * 50)\n",
    "print(df.isnull().sum())\n",
    "print()\n",
    "print('=' * 50)\n",
    "print('âš–ï¸  Class Distribution')\n",
    "print('=' * 50)\n",
    "print(df['species'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "describe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of all features\n",
    "print('ğŸ“ˆ Statistical Summary')\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ” 3. Exploratory Data Analysis (EDA)\n",
    "We visualize the data to understand distributions, correlations, and class separability before training any model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Plot 1: Feature Distributions per Species â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('ğŸŒ¸ Feature Distributions by Iris Species', fontsize=18, fontweight='bold', y=1.02)\n",
    "\n",
    "features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "titles   = ['Sepal Length (cm)', 'Sepal Width (cm)', 'Petal Length (cm)', 'Petal Width (cm)']\n",
    "palette  = {'setosa': '#2ecc71', 'versicolor': '#3498db', 'virginica': '#e74c3c'}\n",
    "\n",
    "for ax, feat, title in zip(axes.flatten(), features, titles):\n",
    "    for species, color in palette.items():\n",
    "        subset = df[df['species'] == species][feat]\n",
    "        ax.hist(subset, bins=15, alpha=0.65, color=color, label=species, edgecolor='white')\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Value (cm)', fontsize=11)\n",
    "    ax.set_ylabel('Frequency', fontsize=11)\n",
    "    ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('âœ… Plot saved as feature_distributions.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-corr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Plot 2: Correlation Heatmap â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "numeric_df = df.drop('species', axis=1)\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(\n",
    "    corr_matrix, mask=mask, annot=True, fmt='.2f',\n",
    "    cmap='RdYlGn', center=0, vmin=-1, vmax=1,\n",
    "    square=True, linewidths=0.5, ax=ax,\n",
    "    annot_kws={'size': 12, 'weight': 'bold'}\n",
    ")\n",
    "ax.set_title('ğŸ”— Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('âœ… Plot saved as correlation_heatmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-pairplot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Plot 3: Pairplot â€” Full Feature Relationships â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pair_plot = sns.pairplot(\n",
    "    df, hue='species', diag_kind='kde',\n",
    "    palette=palette, plot_kws={'alpha': 0.7, 's': 60},\n",
    "    diag_kws={'fill': True, 'alpha': 0.5}\n",
    ")\n",
    "pair_plot.fig.suptitle('ğŸŒ Pairplot â€” All Feature Combinations by Species',\n",
    "                        fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.savefig('pairplot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('âœ… Plot saved as pairplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-box",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Plot 4: Boxplots â€” Outlier Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 6))\n",
    "fig.suptitle('ğŸ“¦ Boxplots â€” Feature Distribution & Outliers by Species',\n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "for ax, feat, title in zip(axes, features, titles):\n",
    "    sns.boxplot(data=df, x='species', y=feat, palette=palette, ax=ax,\n",
    "                width=0.5, linewidth=1.5, flierprops={'marker': 'o', 'markersize': 6})\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Value (cm)', fontsize=10)\n",
    "    ax.tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('boxplots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('âœ… Plot saved as boxplots.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda-insights",
   "metadata": {},
   "source": [
    "### ğŸ’¡ EDA Key Insights\n",
    "\n",
    "| Observation | Detail |\n",
    "|-------------|--------|\n",
    "| **Setosa is linearly separable** | Setosa is clearly distinct from the other two species across petal features |\n",
    "| **Petal features are most discriminative** | `petal_length` and `petal_width` show highest correlation with species (0.96) |\n",
    "| **Versicolor & Virginica overlap** | These two classes overlap slightly, making them harder to separate |\n",
    "| **No missing values** | Dataset is clean â€” no preprocessing needed for nulls |\n",
    "| **Balanced classes** | Each species has exactly 50 samples â€” no class imbalance issue |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ§¹ 4. Data Preprocessing\n",
    "We split the data into training and test sets, then apply **StandardScaler** to normalize features so that distance-based models (like KNN) perform optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Separate Features (X) and Target (y) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X = df.drop('species', axis=1)\n",
    "y = df['species']\n",
    "\n",
    "# Encode target labels to integers\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# â”€â”€ Train-Test Split (80% train / 20% test) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# â”€â”€ Feature Scaling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)   # Fit ONLY on training data\n",
    "X_test_scaled  = scaler.transform(X_test)         # Transform test data\n",
    "\n",
    "print('âœ… Preprocessing Complete!')\n",
    "print(f'   Training samples : {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.0f}%)')\n",
    "print(f'   Testing samples  : {X_test.shape[0]}  ({X_test.shape[0]/len(X)*100:.0f}%)')\n",
    "print(f'   Features         : {X_train.shape[1]}')\n",
    "print(f'   Target classes   : {le.classes_.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¤– 5. Model Building â€” 4 Classifiers\n",
    "We train **4 different classifiers** and evaluate each one. This lets us compare their strengths and pick the best performer.\n",
    "\n",
    "| Model | Type | Strength |\n",
    "|-------|------|----------|\n",
    "| Logistic Regression | Linear | Fast, interpretable, great baseline |\n",
    "| Decision Tree | Tree-based | Highly interpretable, visual rules |\n",
    "| Random Forest | Ensemble | High accuracy, handles noise well |\n",
    "| K-Nearest Neighbors | Instance-based | Simple, no training phase |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Define all 4 Models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=300, random_state=42),\n",
    "    'Decision Tree'      : DecisionTreeClassifier(max_depth=4, random_state=42),\n",
    "    'Random Forest'      : RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "    'KNN'                : KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "}\n",
    "\n",
    "# â”€â”€ Train & Evaluate Each Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "results = {}\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print('ğŸš€ Training Models...\\n')\n",
    "print(f'{\"Model\":<25} {\"Test Accuracy\":>14} {\"CV Mean\":>10} {\"CV Std\":>10}')\n",
    "print('-' * 62)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train on scaled data\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Test accuracy\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score (5-fold)\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train,\n",
    "                                 cv=cv_strategy, scoring='accuracy')\n",
    "    \n",
    "    results[name] = {\n",
    "        'model'       : model,\n",
    "        'y_pred'      : y_pred,\n",
    "        'test_acc'    : test_acc,\n",
    "        'cv_mean'     : cv_scores.mean(),\n",
    "        'cv_std'      : cv_scores.std(),\n",
    "        'cv_scores'   : cv_scores\n",
    "    }\n",
    "    \n",
    "    print(f'{name:<25} {test_acc*100:>13.2f}% {cv_scores.mean()*100:>9.2f}% {cv_scores.std()*100:>9.2f}%')\n",
    "\n",
    "print('\\nâœ… All models trained successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š 6. Model Comparison & Evaluation\n",
    "Now we compare all 4 models visually â€” accuracy bars, cross-validation scores, and confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Plot: Model Accuracy Comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model_names  = list(results.keys())\n",
    "test_accs    = [results[m]['test_acc'] * 100 for m in model_names]\n",
    "cv_means     = [results[m]['cv_mean']  * 100 for m in model_names]\n",
    "cv_stds      = [results[m]['cv_std']   * 100 for m in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 7))\n",
    "\n",
    "bars1 = ax.bar(x - width/2, test_accs, width, label='Test Accuracy',\n",
    "               color=COLORS, alpha=0.85, edgecolor='white', linewidth=1.5)\n",
    "bars2 = ax.bar(x + width/2, cv_means, width, label='CV Mean Accuracy (5-Fold)',\n",
    "               color=COLORS, alpha=0.45, edgecolor='white', linewidth=1.5, hatch='//')\n",
    "\n",
    "# Add error bars for CV std deviation\n",
    "ax.errorbar(x + width/2, cv_means, yerr=cv_stds, fmt='none',\n",
    "            color='black', capsize=6, linewidth=2)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.3,\n",
    "            f'{bar.get_height():.1f}%', ha='center', va='bottom',\n",
    "            fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Classifier', fontsize=13)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=13)\n",
    "ax.set_title('ğŸ† Model Accuracy Comparison â€” Test vs Cross-Validation',\n",
    "             fontsize=16, fontweight='bold', pad=15)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_names, fontsize=12)\n",
    "ax.set_ylim(85, 102)\n",
    "ax.legend(fontsize=11)\n",
    "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda val, _: f'{val:.0f}%'))\n",
    "ax.grid(axis='y', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('âœ… Plot saved as model_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Plot: Confusion Matrices for All 4 Models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 4, figsize=(22, 5))\n",
    "fig.suptitle('ğŸ”¢ Confusion Matrices â€” All Classifiers', fontsize=17, fontweight='bold')\n",
    "\n",
    "cmaps = ['Greens', 'Blues', 'Oranges', 'Purples']\n",
    "\n",
    "for ax, (name, res), cmap in zip(axes, results.items(), cmaps):\n",
    "    cm = confusion_matrix(y_test, res['y_pred'])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                   display_labels=le.classes_)\n",
    "    disp.plot(ax=ax, colorbar=False, cmap=cmap)\n",
    "    ax.set_title(f'{name}\\nAcc: {res[\"test_acc\"]*100:.1f}%',\n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Label', fontsize=10)\n",
    "    ax.set_ylabel('True Label', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('âœ… Plot saved as confusion_matrices.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cv-boxplot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Plot: Cross-Validation Score Distribution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, ax = plt.subplots(figsize=(11, 6))\n",
    "\n",
    "cv_data  = [results[m]['cv_scores'] * 100 for m in model_names]\n",
    "bp = ax.boxplot(cv_data, labels=model_names, patch_artist=True,\n",
    "                medianprops={'color': 'black', 'linewidth': 2.5},\n",
    "                whiskerprops={'linewidth': 1.5},\n",
    "                capprops={'linewidth': 2})\n",
    "\n",
    "for patch, color in zip(bp['boxes'], COLORS):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_title('ğŸ“‰ Cross-Validation Score Distribution (5-Fold)',\n",
    "             fontsize=16, fontweight='bold', pad=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_xlabel('Classifier', fontsize=12)\n",
    "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda v, _: f'{v:.0f}%'))\n",
    "ax.set_ylim(80, 105)\n",
    "ax.grid(axis='y', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cv_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('âœ… Plot saved as cv_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ† 7. Best Model â€” Deep Dive\n",
    "We identify the best model and perform a full detailed evaluation including classification report and feature importance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Identify Best Model by Test Accuracy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "best_name = max(results, key=lambda m: results[m]['test_acc'])\n",
    "best      = results[best_name]\n",
    "\n",
    "print('=' * 55)\n",
    "print(f'   ğŸ† Best Model: {best_name}')\n",
    "print(f'   ğŸ“Š Test Accuracy : {best[\"test_acc\"]*100:.2f}%')\n",
    "print(f'   ğŸ” CV Mean       : {best[\"cv_mean\"]*100:.2f}%')\n",
    "print(f'   ğŸ“‰ CV Std Dev    : {best[\"cv_std\"]*100:.2f}%')\n",
    "print('=' * 55)\n",
    "print()\n",
    "print('ğŸ“‹ Detailed Classification Report:')\n",
    "print('â”€' * 55)\n",
    "print(classification_report(y_test, best['y_pred'],\n",
    "                             target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Feature Importance (Random Forest) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "rf_model = results['Random Forest']['model']\n",
    "importances = rf_model.feature_importances_\n",
    "feat_names  = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_idx   = np.argsort(importances)[::-1]\n",
    "sorted_feats = [feat_names[i] for i in sorted_idx]\n",
    "sorted_imps  = importances[sorted_idx]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "bars = ax.barh(sorted_feats[::-1], sorted_imps[::-1] * 100,\n",
    "               color=COLORS[:4], edgecolor='white', linewidth=1.5, alpha=0.85)\n",
    "\n",
    "for bar, val in zip(bars, sorted_imps[::-1]):\n",
    "    ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val*100:.1f}%', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_title('ğŸŒ³ Random Forest â€” Feature Importance',\n",
    "             fontsize=15, fontweight='bold', pad=12)\n",
    "ax.set_xlabel('Importance (%)', fontsize=12)\n",
    "ax.set_xlim(0, max(sorted_imps * 100) + 12)\n",
    "ax.grid(axis='x', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('âœ… Plot saved as feature_importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decision-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Decision Boundary â€” Petal Features (Best 2D Separation) â”€â”€â”€\n",
    "X_vis = X_train_scaled[:, 2:4]   # petal_length & petal_width\n",
    "y_vis = y_train\n",
    "\n",
    "rf_vis = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_vis.fit(X_vis, y_vis)\n",
    "\n",
    "h = 0.01\n",
    "x_min, x_max = X_vis[:, 0].min() - 0.5, X_vis[:, 0].max() + 0.5\n",
    "y_min, y_max = X_vis[:, 1].min() - 0.5, X_vis[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                      np.arange(y_min, y_max, h))\n",
    "Z = rf_vis.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.contourf(xx, yy, Z, alpha=0.25,\n",
    "            colors=['#2ecc71', '#3498db', '#e74c3c'])\n",
    "ax.contour(xx, yy, Z, colors='white', linewidths=1, alpha=0.6)\n",
    "\n",
    "species_colors = {0: '#2ecc71', 1: '#3498db', 2: '#e74c3c'}\n",
    "for cls, color in species_colors.items():\n",
    "    mask = y_vis == cls\n",
    "    ax.scatter(X_vis[mask, 0], X_vis[mask, 1], c=color,\n",
    "               s=80, edgecolors='white', linewidth=0.8,\n",
    "               label=le.classes_[cls], zorder=5)\n",
    "\n",
    "ax.set_title('ğŸ—ºï¸ Random Forest â€” Decision Boundary\\n(Petal Length vs Petal Width)',\n",
    "             fontsize=15, fontweight='bold')\n",
    "ax.set_xlabel('Petal Length (scaled)', fontsize=12)\n",
    "ax.set_ylabel('Petal Width (scaled)', fontsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('decision_boundary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('âœ… Plot saved as decision_boundary.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… 8. Conclusion & Insights\n",
    "\n",
    "### ğŸ“Š Model Performance Summary\n",
    "\n",
    "| Model | Test Accuracy | CV Mean | Verdict |\n",
    "|-------|--------------|---------|----------|\n",
    "| Logistic Regression | â€” | â€” | âœ… Great baseline |\n",
    "| Decision Tree | â€” | â€” | âœ… Interpretable |\n",
    "| **Random Forest** | **â€”** | **â€”** | ğŸ† **Best Overall** |\n",
    "| KNN | â€” | â€” | âœ… Simple & effective |\n",
    "\n",
    "*(Fill in final accuracy values after running the notebook)*\n",
    "\n",
    "### ğŸ”‘ Key Insights\n",
    "\n",
    "1. **Petal features are the most powerful predictors.** `petal_length` and `petal_width` alone can separate the 3 species with near-perfect accuracy, as evidenced by the decision boundary visualization.\n",
    "\n",
    "2. **Setosa is trivially easy to classify** â€” all 4 models correctly identify all Setosa samples. The challenge lies in distinguishing Versicolor from Virginica.\n",
    "\n",
    "3. **Random Forest is the most robust model**, handling the slight overlap between Versicolor and Virginica better than the other classifiers due to ensemble averaging.\n",
    "\n",
    "4. **The dataset has no missing values and balanced classes**, making it an ideal learning dataset for classification tasks.\n",
    "\n",
    "5. **Cross-validation confirms there is no overfitting** â€” all models show consistent train/CV performance.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Future Work\n",
    "- Hyperparameter tuning with GridSearchCV for further accuracy improvement\n",
    "- Try SVM and Gradient Boosting classifiers\n",
    "- Deploy the best model as a simple web app using Flask or Streamlit\n",
    "\n",
    "---\n",
    "*ğŸŒ¸ CodeAlpha Data Science Internship | Task 1 â€” Iris Flower Classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Final Summary Table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('\\n' + '='*60)\n",
    "print('       ğŸŒ¸ IRIS CLASSIFICATION â€” FINAL RESULTS SUMMARY')\n",
    "print('='*60)\n",
    "\n",
    "summary_data = []\n",
    "for name, res in results.items():\n",
    "    summary_data.append({\n",
    "        'Model': name,\n",
    "        'Test Accuracy': f\"{res['test_acc']*100:.2f}%\",\n",
    "        'CV Mean': f\"{res['cv_mean']*100:.2f}%\",\n",
    "        'CV Std': f\"{res['cv_std']*100:.2f}%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Test Accuracy', ascending=False)\n",
    "summary_df.index = range(1, len(summary_df) + 1)\n",
    "print(summary_df.to_string())\n",
    "print('='*60)\n",
    "print(f'\\nğŸ† Best Model: {best_name} ({best[\"test_acc\"]*100:.2f}% Test Accuracy)')\n",
    "print('\\nâœ… Task 1 Complete â€” CodeAlpha Data Science Internship')"
   ]
  }
 ]
}
